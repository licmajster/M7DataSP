{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9ac2d70-6791-4b1f-b65d-b8c88ce0eeba",
   "metadata": {},
   "source": [
    "# Train your own small GPT-2 model\n",
    "\n",
    "If you want to experiment with the trained model, you can do it at `Inference API` panel of\n",
    "\n",
    "https://huggingface.co/openai-community/gpt2?text=My+name+is+Thomas+and+my+main\n",
    "\n",
    "Note that we are training small GPT2 model on a tiny dataset. Still We can see observe how the model improve with the number of steps and get some interesting results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eeab7707",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate>=0.26.0\n",
      "  Downloading accelerate-1.1.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate>=0.26.0) (0.26.2)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate>=0.26.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate>=0.26.0) (24.1)\n",
      "Requirement already satisfied: psutil in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate>=0.26.0) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate>=0.26.0) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate>=0.26.0) (0.4.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate>=0.26.0) (2.2.1+cu121)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.16.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.9.0)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (1.13.2)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate>=0.26.0) (12.6.68)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate>=0.26.0) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate>=0.26.0) (1.3.0)\n",
      "Downloading accelerate-1.1.1-py3-none-any.whl (333 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94549313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.46.2)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers[torch]) (3.16.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers[torch]) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers[torch]) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers[torch]) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers[torch]) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers[torch]) (2024.11.6)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers[torch]) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers[torch]) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers[torch]) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers[torch]) (4.66.5)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers[torch]) (1.1.1)\n",
      "Requirement already satisfied: torch in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers[torch]) (2.2.1+cu121)\n",
      "Requirement already satisfied: psutil in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate>=0.26.0->transformers[torch]) (6.0.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.12.2)\n",
      "Requirement already satisfied: sympy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->transformers[torch]) (1.13.2)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->transformers[torch]) (3.3)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->transformers[torch]) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->transformers[torch]) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->transformers[torch]) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->transformers[torch]) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->transformers[torch]) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->transformers[torch]) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->transformers[torch]) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->transformers[torch]) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->transformers[torch]) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]) (12.6.68)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers[torch]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers[torch]) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers[torch]) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers[torch]) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy->torch->transformers[torch]) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"transformers[torch]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29f851bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.46.2)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (3.16.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "#!pip install datasets\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df4ce989-ea7f-4c89-975c-046038996d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import wandb  # we will talk about wandb next lecture\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef7cfa7-f664-4ea9-8f04-17ce270272de",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "\n",
    "Before training, we have to tokenize the data and split them into chunks of the same size as context size of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ec1afff-194b-461c-8073-75b769ccc0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your own dataset\n",
    "dataset = load_dataset(\"licmajster/CZ_articles_wiki\")\n",
    "\n",
    "# Make validation split\n",
    "dataset = dataset['train'].train_test_split(test_size=0.0015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db3e836a-8548-496b-bdaf-d2b98f5e2596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the gpt-2 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token=tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cb7a1f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'text', 'source'],\n",
       "        num_rows: 1384\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['title', 'text', 'source'],\n",
       "        num_rows: 3\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82b1dfeb-3191-414e-b6b9-0b198946d334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ca33cabb6e44b7297f721679a1de3b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1148 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b744917966e9446b8f9d11987984b6a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'source', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1384\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['title', 'source', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 3\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize the dataset\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(text=example[\"text\"])\n",
    "tokenized_ds = dataset.map(tokenize_function, batched=True, remove_columns='text')\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b71dd8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  [\n",
      "    [\n",
      "      26705,\n",
      "      24573,\n",
      "      89,\n",
      "      320,\n",
      "      686,\n",
      "      ...\n",
      "      2634,\n",
      "      3211,\n",
      "      6557,\n",
      "      9892,\n",
      "      13\n",
      "    ],\n",
      "    [\n",
      "      40059,\n",
      "      375,\n",
      "      346,\n",
      "      384,\n",
      "      410,\n",
      "      ...\n",
      "      121,\n",
      "      354,\n",
      "      10495,\n",
      "      1309,\n",
      "      13\n",
      "    ],\n",
      "    ...\n",
      "    [\n",
      "      41,\n",
      "      72,\n",
      "      129,\n",
      "      247,\n",
      "      8836,\n",
      "      ...\n",
      "      28026,\n",
      "      7344,\n",
      "      3693,\n",
      "      18,\n",
      "      60\n",
      "    ],\n",
      "    [\n",
      "      7908\n",
      "    ]\n",
      "  ],\n",
      "  [\n",
      "    [\n",
      "      18833,\n",
      "      11223,\n",
      "      8873,\n",
      "      299,\n",
      "      12022,\n",
      "      ...\n",
      "      74,\n",
      "      280,\n",
      "      3693,\n",
      "      20,\n",
      "      60\n",
      "    ],\n",
      "    [\n",
      "      11528\n",
      "    ],\n",
      "    ...\n",
      "    [\n",
      "      49,\n",
      "      11601,\n",
      "      16450,\n",
      "      1976,\n",
      "      1408,\n",
      "      ...\n",
      "      13038,\n",
      "      73,\n",
      "      3693,\n",
      "      19,\n",
      "      60\n",
      "    ],\n",
      "    [\n",
      "      49,\n",
      "      11601,\n",
      "      24648,\n",
      "      416,\n",
      "      75,\n",
      "      ...\n",
      "      249,\n",
      "      75,\n",
      "      344,\n",
      "      76,\n",
      "      13\n",
      "    ]\n",
      "  ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_ds[\"train\"].data[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98cdb38c-eb4f-47e7-8c43-96bacb7826ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids'],\n",
      "        num_rows: 446\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "def concatenate_and_chunk(dataset, chunk_size=512):\n",
    "    # Flatten all `input_ids` into a single list\n",
    "    all_input_ids = list(chain(*dataset[\"input_ids\"]))\n",
    "    \n",
    "    # Create chunks of `chunk_size`\n",
    "    chunks = [all_input_ids[i:i + chunk_size] for i in range(0, len(all_input_ids), chunk_size)]\n",
    "    \n",
    "    # Only keep chunks that are exactly of length `chunk_size`\n",
    "    # chunks = [chunk for chunk in chunks if len(chunk) == chunk_size]\n",
    "    if len(chunks[-1]) != chunk_size:\n",
    "        chunks.pop()\n",
    "    \n",
    "    # Create a new dataset with only the `input_ids` chunks\n",
    "    return Dataset.from_dict({\"input_ids\": chunks})\n",
    "\n",
    "# Apply this function to each split (train and test) in the DatasetDict\n",
    "chunked_ds = DatasetDict({\n",
    "    split: concatenate_and_chunk(split_ds, chunk_size=512)\n",
    "    for split, split_ds in tokenized_ds.items()\n",
    "})\n",
    "\n",
    "print(chunked_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "426c29a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InMemoryTable\n",
      "input_ids: list<item: int32>\n",
      "  child 0, item: int32\n",
      "----\n",
      "input_ids: [[[26705,24573,89,320,686,...,270,1424,74,2634,8873],[279,709,349,21162,8836,...,479,353,2634,384,299],...,[978,32790,709,128,249,...,11,285,1219,315,77],[2634,264,2100,88,257,...,20259,13139,9038,1477,1659]]]\n"
     ]
    }
   ],
   "source": [
    "print(chunked_ds[\"train\"].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36f1ef42-f488-4ca0-af36-464f3e535be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data collator joins chunks into batches\n",
    "# see https://huggingface.co/docs/transformers/en/main_classes/data_collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe26fe6-925f-49ec-a854-ebdb429c66ad",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37301b62-4bea-4006-aa1a-7785924227b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model configuration for the smallest GPT-2\n",
    "config = GPT2Config(\n",
    "    vocab_size=len(tokenizer),      # Standard GPT-2 vocab size 50257\n",
    "    n_positions=512,                # Context size (512 is enough for small-scale models)\n",
    "    n_embd=768,                     # Embedding size\n",
    "    n_layer=12,                     # Number of transformer layers\n",
    "    n_head=12,                      # Number of attention heads\n",
    ")\n",
    "\n",
    "# Initialize the model and tokenizer\n",
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7a48b42-9ce2-433f-acff-ce694449c317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Define the perplexity metric\n",
    "def compute_metrics(eval_pred):\n",
    "    # `eval_pred` is a tuple of (logits, labels)\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    # Convert logits and labels to PyTorch tensors if they are NumPy arrays\n",
    "    if isinstance(logits, np.ndarray):\n",
    "        logits = torch.tensor(logits)\n",
    "    if isinstance(labels, np.ndarray):\n",
    "        labels = torch.tensor(labels)\n",
    "\n",
    "    # Shift labels so that tokens align for calculating loss\n",
    "    shift_labels = labels[:, 1:].reshape(-1)\n",
    "    shift_logits = logits[:, :-1, :].reshape(-1, logits.shape[-1])\n",
    "\n",
    "    # Calculate the cross-entropy loss\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)  # Ignore padding tokens\n",
    "    loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "    # Calculate perplexity\n",
    "    perplexity = math.exp(loss.item())\n",
    "    return {\"perplexity\": perplexity}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff4c203-9d5d-494e-b425-4b5a265516ff",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b1ff634b-e879-44ad-a6d1-21c0a573aa0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40883/659259037.py:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(model=model,\n"
     ]
    }
   ],
   "source": [
    "# Set this according to size of your dataset\n",
    "# You should train for at least 15 mins on A10 GPU to get something reasonable\n",
    "TRAIN_EPOCHS = 30\n",
    "\n",
    "SAVE_STEPS = 100\n",
    "EVAL_STEPS = SAVE_STEPS // 2\n",
    "\n",
    "# training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-training\",  # Directory to save the model checkpoints and other outputs\n",
    "    eval_strategy=\"steps\",  # Evaluation strategy to use during training ('steps' or 'epochs')\n",
    "    eval_steps=EVAL_STEPS,  # Perform evaluation every EVAL_STEPS steps\n",
    "    num_train_epochs=TRAIN_EPOCHS,  # Total number of training epochs\n",
    "    per_device_train_batch_size=16,  # Batch size for training on each device\n",
    "    per_device_eval_batch_size=16,  # Batch size for evaluation on each device\n",
    "    # learning_rate=2.5e-4,  # Initial learning rate for the optimizer\n",
    "    learning_rate=1e-4,  # Initial learning rate for the optimizer\n",
    "    lr_scheduler_type='cosine',  # Learning rate scheduler type. 'cosine' provides a cosine decay schedule.\n",
    "    warmup_ratio=0.05,  # Proportion of training to perform linear learning rate warmup for\n",
    "    adam_beta1=0.9,  # Beta1 parameter for the Adam optimizer (first moment decay)\n",
    "    adam_beta2=0.999,  # Beta2 parameter for the Adam optimizer (second moment decay)\n",
    "    weight_decay=0.01,  # Weight decay to apply (L2 regularization)\n",
    "    logging_strategy=\"steps\",  # Logging strategy to use. 'steps' logs at specified steps.\n",
    "    logging_steps=EVAL_STEPS,  # Log training metrics every EVAL_STEPS steps\n",
    "    save_steps=SAVE_STEPS,  # Save a checkpoint every SAVE_STEPS steps\n",
    "    save_total_limit=10,  # Maximum number of checkpoints to keep. Older checkpoints are deleted.\n",
    "    # report_to='wandb',  # Uncomment to report metrics to Weights and Biases (optional)\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                 args = training_args,\n",
    "                 tokenizer=tokenizer,\n",
    "                 train_dataset=chunked_ds[\"train\"],\n",
    "                 eval_dataset=chunked_ds[\"test\"],\n",
    "                 compute_metrics=compute_metrics,\n",
    "                 data_collator = data_collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f72a95fd-d7ac-4403-81d8-a42532f7a7a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='840' max='840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [840/840 15:22, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.178200</td>\n",
       "      <td>3.760219</td>\n",
       "      <td>42.957417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.003600</td>\n",
       "      <td>3.808624</td>\n",
       "      <td>45.087732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.835300</td>\n",
       "      <td>3.907951</td>\n",
       "      <td>49.796083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.690300</td>\n",
       "      <td>3.958899</td>\n",
       "      <td>52.398754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.552500</td>\n",
       "      <td>4.001725</td>\n",
       "      <td>54.691355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.373500</td>\n",
       "      <td>4.130373</td>\n",
       "      <td>62.199963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.190900</td>\n",
       "      <td>4.275305</td>\n",
       "      <td>71.900680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.009800</td>\n",
       "      <td>4.385308</td>\n",
       "      <td>80.261507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.838100</td>\n",
       "      <td>4.427083</td>\n",
       "      <td>83.685311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.671800</td>\n",
       "      <td>4.586266</td>\n",
       "      <td>98.125330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.537200</td>\n",
       "      <td>4.655647</td>\n",
       "      <td>105.175120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.421800</td>\n",
       "      <td>4.708323</td>\n",
       "      <td>110.864020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.333100</td>\n",
       "      <td>4.763983</td>\n",
       "      <td>117.209646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.271800</td>\n",
       "      <td>4.788507</td>\n",
       "      <td>120.119772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.228200</td>\n",
       "      <td>4.811377</td>\n",
       "      <td>122.898573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.204700</td>\n",
       "      <td>4.814348</td>\n",
       "      <td>123.264093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=840, training_loss=1.9819635663713728, metrics={'train_runtime': 923.4019, 'train_samples_per_second': 14.49, 'train_steps_per_second': 0.91, 'total_flos': 3496087388160000.0, 'train_loss': 1.9819635663713728, 'epoch': 30.0})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "54b62744-896e-4faf-b21b-3e031dc1217d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./gpt2-small-final\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01639456-2124-46e8-8d6c-46b4cbd5c484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eaa187932ca46b3938778a2d0c36c72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/496M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "095a840811c447848ad82b479f7e9efc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/licmajster/my_small_gpt2_cswiki/commit/f851ab2d198d96620d8bb33088572f4bce835971', commit_message='Upload tokenizer', commit_description='', oid='f851ab2d198d96620d8bb33088572f4bce835971', pr_url=None, repo_url=RepoUrl('https://huggingface.co/licmajster/my_small_gpt2_cswiki', endpoint='https://huggingface.co', repo_type='model', repo_id='licmajster/my_small_gpt2_cswiki'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YOUR_MODEL_NAME = \"my_small_gpt2_cswiki\" # change this\n",
    "HF_TOKEN = \"TOKEN\"  # change this \n",
    "\n",
    "model.push_to_hub(YOUR_MODEL_NAME, token=HF_TOKEN)\n",
    "tokenizer.push_to_hub(YOUR_MODEL_NAME, token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd6dac4-f46e-42a2-833f-07cb16eab3bc",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Now you can switch from GPU to CPU. Try to complete some prompt specific to your dataset.\n",
    "\n",
    "Does it make sense? Is it at least in Czech/Slovak?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0f9ade4a-f8b3-441d-8b6f-550048a21f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  GPT2LMHeadModel, AutoTokenizer, pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token=tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f9ee0298-2d7f-4aa4-9a2d-13429b7e197e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "model =  GPT2LMHeadModel.from_pretrained(\"./gpt2-small-final\")\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e42697e6-10ad-4ea8-a9a9-addef655a1d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Počas války v letechze studoval na Akademii v letech 1945–1955–1955–1955–1956 na Akademii výtvarných umělecké'},\n",
       " {'generated_text': 'Počas války v letechze studoval na Akademii v ateliéru prof. V letech 1949–1946 byl školu výtvarných uměn'},\n",
       " {'generated_text': 'Počas války v letechze studoval na Akademii v ateliéru prof. V letech 1949–1938 studia na Akademii výtvarných uměleckop'}]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROMPT = \"Počas války v letech\" # Set starting prompt, something specific for your dataset\n",
    "\n",
    "generator(\n",
    "    PROMPT,\n",
    "    max_length=50,       # Maximum length of the generated text\n",
    "    do_sample=True,\n",
    "    temperature=0.3,         # Experiment with this\n",
    "    repetition_penalty=0.9,  # Experiment with this\n",
    "    num_return_sequences=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b12a36-09d5-4e3b-99bd-1ea157012107",
   "metadata": {},
   "source": [
    "Now go back to your training folder `.gpt2-training/`. Each `checkpoint-N` folder contains the model saved after N steps. \n",
    "\n",
    "If you experiment with the older models, you should see that the models improves with time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8e7a20e6-c47b-44e5-8d1a-2e6825dbef74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_after_N_steps(N, prompt, **kwargs):\n",
    "    model =  GPT2LMHeadModel.from_pretrained(f\"./gpt2-training/checkpoint-{N}/\")\n",
    "    generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    output = generator(prompt, **kwargs)\n",
    "    return output  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7fc850f1-9c94-4a71-b56b-a2eecc5eecf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Pokus výtvarného umění.[1] V ro'}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sample_after_N_steps(200, \"Pokus\", do_sample=True, temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "035fbb7a-4147-435f-b9fc-f3c30a6eea1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Pokus, kterém ateliér. V roce 1975 záž'}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sample_after_N_steps(400, \"Pokus\", do_sample=True, temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b64f2fe5-9ada-497b-aff0-10b0bb327785",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Pokus, uře vychátce se kdešskal'}]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sample_after_N_steps(600, \"Pokus\", do_sample=True, temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f4459182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting simpletransformers\n",
      "  Downloading simpletransformers-0.70.1-py3-none-any.whl.metadata (42 kB)\n",
      "Requirement already satisfied: numpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from simpletransformers) (1.26.4)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from simpletransformers) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.47.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from simpletransformers) (4.66.5)\n",
      "Requirement already satisfied: regex in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from simpletransformers) (2024.11.6)\n",
      "Requirement already satisfied: transformers>=4.31.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from simpletransformers) (4.46.2)\n",
      "Requirement already satisfied: datasets in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from simpletransformers) (3.1.0)\n",
      "Requirement already satisfied: scipy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from simpletransformers) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from simpletransformers) (1.3.2)\n",
      "Collecting seqeval (from simpletransformers)\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tensorboard in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from simpletransformers) (2.15.1)\n",
      "Collecting tensorboardx (from simpletransformers)\n",
      "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: pandas in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from simpletransformers) (2.1.4)\n",
      "Requirement already satisfied: tokenizers in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from simpletransformers) (0.20.3)\n",
      "Collecting wandb>=0.10.32 (from simpletransformers)\n",
      "  Downloading wandb-0.18.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
      "Collecting streamlit (from simpletransformers)\n",
      "  Downloading streamlit-1.40.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting sentencepiece (from simpletransformers)\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers>=4.31.0->simpletransformers) (3.16.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers>=4.31.0->simpletransformers) (0.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers>=4.31.0->simpletransformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers>=4.31.0->simpletransformers) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers>=4.31.0->simpletransformers) (0.4.5)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (8.1.7)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb>=0.10.32->simpletransformers)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb>=0.10.32->simpletransformers)\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (4.3.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (4.23.4)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (6.0.0)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb>=0.10.32->simpletransformers)\n",
      "  Downloading sentry_sdk-2.18.0-py2.py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting setproctitle (from wandb>=0.10.32->simpletransformers)\n",
      "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: setuptools in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (72.1.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->simpletransformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->simpletransformers) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->simpletransformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->simpletransformers) (2024.8.30)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets->simpletransformers) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets->simpletransformers) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets->simpletransformers) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets->simpletransformers) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->simpletransformers) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets->simpletransformers) (3.10.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->simpletransformers) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->simpletransformers) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->simpletransformers) (2024.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scikit-learn->simpletransformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scikit-learn->simpletransformers) (3.5.0)\n",
      "Collecting altair<6,>=4.0 (from streamlit->simpletransformers)\n",
      "  Downloading altair-5.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting blinker<2,>=1.0.0 (from streamlit->simpletransformers)\n",
      "  Downloading blinker-1.8.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from streamlit->simpletransformers) (5.5.0)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from streamlit->simpletransformers) (10.4.0)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from streamlit->simpletransformers) (13.8.1)\n",
      "Collecting tenacity<10,>=8.1.0 (from streamlit->simpletransformers)\n",
      "  Downloading tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit->simpletransformers)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit->simpletransformers)\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from streamlit->simpletransformers) (6.4.1)\n",
      "Collecting watchdog<6,>=2.1.5 (from streamlit->simpletransformers)\n",
      "  Downloading watchdog-5.0.3-py3-none-manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: absl-py>=0.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard->simpletransformers) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard->simpletransformers) (1.66.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard->simpletransformers) (2.34.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard->simpletransformers) (1.2.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard->simpletransformers) (3.7)\n",
      "Requirement already satisfied: six>1.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard->simpletransformers) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard->simpletransformers) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard->simpletransformers) (3.0.4)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit->simpletransformers) (3.1.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit->simpletransformers) (4.23.0)\n",
      "Collecting narwhals>=1.5.2 (from altair<6,>=4.0->streamlit->simpletransformers)\n",
      "  Downloading narwhals-1.13.3-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets->simpletransformers) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets->simpletransformers) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets->simpletransformers) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets->simpletransformers) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets->simpletransformers) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets->simpletransformers) (1.11.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets->simpletransformers) (4.0.3)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers)\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->simpletransformers) (2.0.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit->simpletransformers) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit->simpletransformers) (2.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard->simpletransformers) (2.1.5)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers)\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (0.20.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->simpletransformers) (0.1.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->simpletransformers) (3.2.2)\n",
      "Downloading simpletransformers-0.70.1-py3-none-any.whl (316 kB)\n",
      "Downloading wandb-0.18.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.1/16.1 MB\u001b[0m \u001b[31m194.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m136.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading streamlit-1.40.0-py2.py3-none-any.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m159.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "Downloading altair-5.4.1-py3-none-any.whl (658 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m658.1/658.1 kB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading blinker-1.8.2-py3-none-any.whl (9.5 kB)\n",
      "Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m149.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentry_sdk-2.18.0-py2.py3-none-any.whl (317 kB)\n",
      "Downloading tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading watchdog-5.0.3-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
      "Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Downloading narwhals-1.13.3-py3-none-any.whl (201 kB)\n",
      "Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=8e70e49ce7b9efd24a53865692dfcb3865c72ae6df2eff32562d8a19863b8cfd\n",
      "  Stored in directory: /home/zeus/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
      "Successfully built seqeval\n",
      "Installing collected packages: sentencepiece, watchdog, toml, tensorboardx, tenacity, smmap, setproctitle, sentry-sdk, narwhals, docker-pycreds, blinker, pydeck, gitdb, seqeval, gitpython, wandb, altair, streamlit, simpletransformers\n",
      "Successfully installed altair-5.4.1 blinker-1.8.2 docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 narwhals-1.13.3 pydeck-0.9.1 sentencepiece-0.2.0 sentry-sdk-2.18.0 seqeval-1.2.2 setproctitle-1.3.3 simpletransformers-0.70.1 smmap-5.0.1 streamlit-1.40.0 tenacity-9.0.0 tensorboardx-2.6.2.2 toml-0.10.2 wandb-0.18.6 watchdog-5.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install simpletransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0f692c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8751e5a8691e455eace7f303d15dd183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.22k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63125f22d97447e4bc8955f2b20dcd6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e6f327860340ffb635ef8cde278f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/477 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b287ce9f818b4a0486ba1e48b07c3718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d0595619f794e4eb6d626223d8d8835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ca5d731e4a4dd592d769a7832ebfb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/simpletransformers/classification/classification_model.py:484: UserWarning: use_multiprocessing automatically disabled as xlmroberta fails when using multiprocessing for feature conversion.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from simpletransformers.classification import ClassificationModel\n",
    "model_args= {\n",
    "            \"num_train_epochs\": 15,\n",
    "            \"learning_rate\": 1e-5,\n",
    "            \"max_seq_length\": 512,\n",
    "            \"silent\": True\n",
    "            }\n",
    "model = ClassificationModel(\n",
    "    \"xlmroberta\", \"classla/xlm-roberta-base-multilingual-text-genre-classifier\", use_cuda=True,\n",
    "    args=model_args\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "98e3bc3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'text', 'source'],\n",
       "        num_rows: 1384\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['title', 'text', 'source'],\n",
       "        num_rows: 3\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "156c6930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Andrej Kuc (27. listopadu 1919, Lechnica, ČSR – 18. října 1998, Spišská Nová Ves, Slovensko) byl slovenský akademický malíř a restaurátor. Ještě v Lechnici absolvoval osm tříd Ľudové školy. Od dětství se zajímal o malířství, ale kvůli chudobě rodičů si nemohl studia dovolit. Ve věku 16 let se proto odešel vyučit malířskému řemeslu. Jako učeň působil od 14. srpna 1935 do 14. srpna 1938 u malířského mistra Františka Crháka ve Spišské Staré Vsi. Po ukončení výuky začal pracovat jako pomocník pod vedením malířského mistra Ernesta Waltera v Kežmarku, později u malířského mistra Štěpána Palubiaka v Popradu. V té době se také úspěšně podrobil zkouškám ze čtyř tříd měšťanské školy. Rodina na Zamaguří ve skromných podmínkách hospodařila na šesti hektarech polí, luk a lesa. Po ukončení studia pracoval jako svobodný umělec. Na rozsáhlejší vlastní malířskou tvorbu Andrej Kuc neměl čas, celý svůj život zasvětil obnově památek. Pracoval doma i v terénu a nejčastěji restauroval nástěnné malby. Andrej Kuc se narodil v obci Lechnica, okres Poprad. Matka, Alžběta, rozená Lainsová se starala o domácnost. Otec, Jan Kanty Kuc, byl řezbářem a pozlacovačem. Oženil se po skončení první světové války jako 49letý a později si zřídil truhlářskou živnost, aby za prací už nemusel cestovat. Andrej se v mladém věku u otce naučil pozlačovačství, částečně malířství, truhlářství a poměrně dobře řezbářství. Na toto období vzpomínal jako 74letý: Vojenskou službu, během níž deset měsíců působil na východní frontě v SSSR, absolvoval v období od 10. ledna 1939 do 30. července 1942. Ženijní kurz absolvoval jako minér a pyrotechnik. Po skončení výcviku se stal členem četnického sboru („Zbor žandárstva“). Zkoušku absolvoval v četnické škole v Topoľčiankach a následně sloužil jako podřízený četník na stanici Štiavnické Bane. Andrej Kuc se politicky neangažoval. O členství ve KSČ se přestal ucházet v roce 1950. Andreji Kucovi bylo uděleno mnoho vyznamenání, za zásluhy a hrdinství při obnovení Československé republiky a osvobození slovenského národa dostal 9. října 1946 Řád Slovenského národního povstání I. třídy. 27. května 1968 mu bylo za prokázání chrabrého činu v nebezpečí života uděleno vyznamenání Československý válečný kříž 1939. V letech 1959 až 1961 nakrátko působil jako vedoucí Galerie v Bojnicích. Od 1. června 1973 pracoval pro Slovenský ústav památkové péče a ochrany přírody (SÚPSOP), předchůdce dnešního Památkového úřadu Slovenské republiky (PÚSR). Počínaje rokem 1976 přestoupil do podniku Umelecké remeslá (Umělecká řemesla) v Bratislavě, kde byl vedoucím skupiny a ateliéru. Po zřízení Státních restaurátorských ateliérů (SRA) se stal vedoucím ateliéru a vedl přidělené pracovníky zejména při restaurování přenosných uměleckých děl. VŠVU ukončil s vyznamenáním v oboru malířských a konzervačních technik 23. června 1956 u profesora Karla Veselého. Andrej Kuc se poprvé oženil v roce 1946. Jeho manželka Emília, roz. Čamajová, pracovala v Banské Štiavnici jako pletařka. Po osmnácti letech bezdětného manželství zemřela v roce 1964 na následky urémie. V tomto článku byl použit překlad textu z článku Andrej Kuc na slovenské Wikipedii. Osud Andreje Kuca připomíná životní příběh Alexeje Petroviče Maresjeva, který se stal námětem mezinárodně známé novely Borise Polevoje Příběh opravdového člověka. Maresjev byl sovětský letec, který se po pádu stroje během Velké vlastenecké války ocitl s rozdrcenými chodidly v nepřátelském týlu. Přesto se mu během osmnácti dní podařilo vrátit na území kontrolované sovětskou armádou. V důsledku zranění mu museli být amputovány obě nohy pod koleny. Četnický sbor opustil po vypuknutí Slovenského národního povstání. Do bojů se jako polní četník zapojil u Vrútek, Karvinské, Martina a později u Červené Skaly. Po částečném potlačení povstání se připojil k partyzánskému odboji. Jako velitel roty sloužil ve Druhé partyzánské brigádě „Za svobodu Slovanů“. Zde bojoval až do příchodu Rudé armády v únoru 1945. Byl přímým účastníkem Slovenského národního povstání. Jako velitel partyzánské roty bojoval až do příjezdu Rudé Armády. Za zásluhy v národně osvobozovacím boji proti fašismu mu byl udělen Řád Slovenského národního povstání I. třídy a Československý válečný kříž 1939. V roce 1965 Andrej Kuc svůj úděl popsal slovy: Podruhé se oženil 8. srpna 1964 ve Spišské Nové Vsi s Marií, rod. Joppovou. Marie v té době pracovala jako úřednice na ředitelství Železorudných dolů. Spolu mají dvě děti - dceru Gabrielu, vdanou Zahurancovou (* 1965) a syna Ľubomíra (* 1969), který v otcových šlépějích pokračuje v restaurátorské činnosti. Po osvobození zjistil, že obyvatelé obce Kjastava pod Sitnem nemohou hospodařit na německými minami kontaminované části katastru. Rozhodl se dobrovolně oblast odminovat. Navzdory mimořádné opatrnosti šlápl dne 4. dubna 1945 na nášlapnou minu a utrpěl devastační poranění obou nohou v oblasti bérce, v důsledku čehož mu byly obě nohy amputovány. Ke studiu oboru restaurování uměleckých památek na Vysoké škole výtvarných umění v Bratislavě byl přijat ve stejném roce, 1. října 1951. Popsal ho jako těžké: Během své činnosti jako restaurátor restauroval a zakonzervoval množství památek různého rázu - gotické i barokní nástěnné a olejové malby, různé plastiky, kamenořezby a sgrafita. Po vyléčení pracoval od 27. listopadu 1949 jako tajemník OAV-SNF (Okresního akčního výboru Slovenské národního fronty) a jako vedoucí zemědělského skladištního družstva v Spišské Staré Vsi až do 30. listopadu 1950. Později, od 1. února 1951, pracoval nakrátko jako grafik v národním podniku Obuva v Partizánském. Andrej Kuc svůj život zasvětil obnově slovenských uměleckých památek. Původní podobu vrátil více než stovce významných děl ve všech koutech země. ', 'Po druhé světové válce se angažoval politicky a zastával vedoucí pozice v slovenské i československé komunistické straně. V letech 1953–1976 se uvádí jako člen Ústředního výboru Komunistické strany Slovenska.[3] V letech 1964–1968 byl zároveň členem předsednictva ÚV KSS. 11. sjezd KSČ ho také zvolil za člena Ústředního výboru Komunistické strany Československa. Ve funkci ho potvrdil 12. sjezd KSČ, 13. sjezd KSČ, 14. sjezd KSČ a 15. sjezd KSČ.[4] V letech 1948–1968 a znovu od roku 1972 byl předsedou Ústředního výboru organizace maďarské národnostní menšiny v Československu Csemadok. K roku 1954 se navíc profesně uvádí jako redaktor listu Uj szó. Od roku 1977 byl předsedou Svazu slovenských výtvarných umělců a od roku 1978 i místopředsedou Svazu československých výtvarných umělců. Byl nositelem četných státních vyznamenání. V roce 1960 mu byl udělen Řád práce, roku 1973 Řád Vítězného února, roku 1977 Leninova státní cena. V roce 1975 získal titul zasloužilý umělec, v roce 1980 pak i národní umělec.[4][11] V mládí se profiloval jako umělec – malíř. Studoval na soukromých školách G. Mallého v Bratislavě a K. Harmose v Komárně. V letech 1929–1934 absolvoval Vysokou školu výtvarných umění v Budapešti u profesorů Rétiho a Vaszaryho. V roce 1935 přesídlil do Bratislavy. Toho roku měl v Bratislavě svou první výstavu. Byl tehdy členem Masarykovy akademie, která mu udělila za výstavu stipendium a zlatou medaili. Díky tomu pak prodělal studijní pobyty v Paříži, Belgii a Nizozemsku. Na přelomu let 1938–1939 odešel do emigrace do Paříže, ale koncem roku 1939 se přestěhoval do Budapešti, kde byl vězněný a žil zde pod setrvalým policejním dozorem po celou válku. Žánrově ho ovlivnil expresionismus a dílo Pabla Picassa, s nímž se osobně znal.[2] Július Lörincz (16. ledna 1910 Diosek – 14. prosince 1980 Bratislava[1]) byl slovenský a československý malíř, politik Komunistické strany Slovenska maďarské národnosti, poúnorový poslanec Národního shromáždění ČSR a Národního shromáždění ČSSR a poslanec Sněmovny lidu Federálního shromáždění za normalizace. Po desítky let zasedal i v nejvyšších zákonodárných sborech. Ve volbách roku 1954 byl zvolen do Národního shromáždění za KSS ve volebním obvodu Fiľakovo-Lučenec-Rimavská Sobota. Mandát obhájil ve volbách v roce 1960 (nyní již jako poslanec Národního shromáždění ČSSR za Středoslovenský kraj, podílel se na přípravě nové ústavy z roku 1960) a volbách v roce 1964. V Národním shromáždění zasedal až do konce jeho funkčního období v roce 1968.[5][6][7] Jeho kariéra pokračovala i po invazi vojsk Varšavské smlouvy do Československa za nastupující normalizace. Po federalizaci Československa usedl roku 1969 za KSS do Sněmovny lidu Federálního shromáždění (volební obvod Veľký Krtíš). Mandát obhájil ve volbách v roce 1971 a volbách v roce 1976 (volební obvod Zemianska Olča). V parlamentu setrval až do své smrti roku 1980.[8][9][10] ', 'Je zastoupen v mnoha soukromých i veřejných sbírkách.[3] Věnuje se malbě, grafice, kresbě, ilustraci a známkové tvorbě. Tvoří postmoderní malby, kdy vychází z abstrakce, surrealismu nebo hyperrealismu.[5] Kreslí také figurální grafiky technikou suché jehly a leptu.[2] Hlásí se ke stylu a odkazu Zdeňka Buriana.[6] Igor Piačka (* 8. října 1962, Třebíč) je slovenský malíř. Igor Piačka se narodil do československé rodiny v Třebíči, vyrůstal však na Slovensku ve městě Vrbové.[2] Mezi lety 1983 a 1989 studoval na Akadémii výtvarných umení v Bratislave v ateliéru profesora Albína Brunovského. Mezi lety 1987 a 1988 studoval na Academie Royale des Beaux Arts v Bruselu. Od roku 1990 do roku 2003 působil jako odborný asistent na Vysokej škole výtvarných umení v Bratislavě, od roku 2003 pak působí jako umělec na volné noze.[3] Vystavoval od roku 1990 na více než 100 samostatných výstavách a na mnoha společných. Získal více než 40 ocenění za grafiku nebo malbu. Navrhl také 27 poštovních známek, 83 ex libris a 60 ilustrovaných knih nebo obálek.[3] Je členem skupin G-bod a spolku Sdružení českých umělců Hollar.[7] Roku 2019 vystavoval poprvé ve svém rodišti, v Třebíči, společně se svou manželkou.[6] Jeho manželkou je Júlia Piačková.[4] Společně vystavoval na Slovensku i v zahraničí, např. v Barceloně (1994), Londýně (1996), Torontu (1997), Washingtonu (1998), Pekingu (2003) i jinde, autorsky pak mimo jiné v Bukurešti (1997), Skandenborgu (2004), New Yorku (2013) nebo v\\xa0Jakartě (2013).[8] ', 'Juraj Krén (26.\\xa0dubna 1925 Nové Mesto nad Váhom – 9.\\xa0září 1969 Bratislava) byl slovenský výtvarník.[1] Malbě se Krén věnoval nejprve pod vedením Petera Matejky a pak,[2] mezi roky 1945 a 1950,[3] na pražské Akademii výtvarných umění (AVU), kde studoval u Vlastimila Rady a Vladimíra Sychry.[2] Během pražských studií byl osloven tvorbou Františka Tichého.[4] Po pražských studiích se navrátil zpět do svého rodného města, kde pobýval do roku 1960, než se přestěhoval do Bratislavy.[3] Příklady Krénova díla: Vlivem svého předčasného úmrtí se Krén věnoval umělecké tvorbě necelých třináct let.[5] V\\xa0ní se věnuje figurálním motivům, dále krajinám a různým zátiším, ale nevynechává ani akty. Jako poctu malíři Tichému vytvořil Krén v\\xa0letech 1960 až 1961 několik variací na Tichého obraz nazvaný Červené eso.[3] Krén rovněž vytvořil tapiserii pojmenovanou Javor a lípa, která visela v\\xa0československém pavilonu na Světové výstavě (EXPO) 1967 v\\xa0Montréalu. Výtvor 16.\\xa0května 1967 předal na závěr své prohlídky objektu československý prezident Antonín Novotný zástupcům Kanady, kteří po skončení výstavy uložili dílo do sbírek Kanadské národní galerie v\\xa0Ottawě.[6] ', 'Od přelomu 70. a 80. let tvoří objekty ze skládaných broušených skleněných hranolů, pro které je charakteristická preciznost zpracování a geometrický řád. Jednotlivé díly jeho objektů jsou k sobě volně přiloženy bez lepení, aby byla zachována propustná zrcadla styčných ploch.[20][21] Matematické vztahy a prostorové formy těchto objektů vytvářejí s měnícím se úhlem pohledu kaleidoskopické škály tvarových variant a spektrálních fines. V cyklu Transcendence (1980-1981) kombinoval čiré optické sklo s barevným opálovým sklem. Asymetrie hranolu z opálového skla navíc nabízí proměnlivou intenzitu barvy.[13] V jeho broušených objektech, skládaných z několika různobarevných hranolů, dochází působením světla ke kinetickým jevům, které představují zhuštěný obraz intuitivně pochopeného přírodního procesu.[14] Těžištěm práce Pavla Trnky jsou studené techniky - řezané, pískované a broušené sklo, skládané objekty a vitráže. Pracuje i s horkými technikami - ručně foukaným sklem a sklem foukaným do formy, vrstveným a litým sklem.[10] Už v 70. letech se prosadil broušenými skleněnými plastikami z optického skla.[11][12] Jeho tvůrčí projev charakterizuje mimořádný cit pro vlastnosti materiálu, se kterým pracuje.[13] Příznačným rysem jeho umělecké povahy je odvaha experimentovat a preference děje, pohybu prolínání a proměny i polarita konstrukce-destrukce.[14] Trnkův přístup k broušenému sklu přinesl inovaci zejména ve skládání nebo rozkladu světelného spektra v barevných objektech.[15] Jeho objekt Residence of Love ocenila porota v Kanazavě roku 2007 Zlatou medailí a vyzdvihla přitom jeho unikátní způsob zacházení se světlem a barvou skla.[16] Roku 1980 vytvořil na symposiu ve sklárně Škrdlovice sérii objektů z hutního skla s nálepy.[17] Jeho rané \"Ne-nádoby\" (1981-1982) z technického křemíkového skla, vytvořené s pomocí vodíkového hořáku, se vyznačovaly zásahy, které narušovaly jejich symetrii a propůjčily jim dynamický \"anarchistický\" detail ve formě původního tekutého skla.[18] Trnka tak natavením nebo nastřižením narušil harmonii neosobního a perfektního tvaru válce a vnesl do něj prvek lidské kreativity a odlehčující humor. Ze stejné doby pochází cyklus „Vyrušené napětí“ (1981-1984), ve kterém si ověřil jak možnosti hutního tvarování skla, tak opracování povrchu skla dlátem a štípáním.[19] V letech 1974-1976 a 1996-1997 vedl experimentální výuku na základních školách, 1997-1999 na Základní umělecké škole v Praze 4.[3] Od roku 2000 působil jako pedagog designu, kreslení a modelování na Střední uměleckoprůmyslové škole sklářské v Kamenickém Šenově. Roku 2004 odcestoval do Japonska a až do roku 2008 byl profesorem na Toyama City Institute of Glass Art, kde vyučoval studené sklářské techniky.[4] Navázal tak na zavedenou tradici výuky studených sklářských technik českými skláři, na jejímž počátku byl roku 1991 Vladimír Klein.[5] Roku 2008 vedl workshop ve Španělsku na Real Fábrica de Cristales de la Granja v San Idelfonso.[6] V patnácti letech ho silně ovlivnily obrazy Františka Kupky na retrospektivní výstavě, kterou roku 1963 uspořádala paní Kupková na MNV v Dobrušce.[1] Současně vnímal intenzivně i krásu funkcionalistické architektury v blízkém Hradci Králové, která byla na počátku jeho pozdějšího příklonu k čistým geometrickým tvarům ve vlastní tvorbě.[2] V letech 1963-1967 absolvoval Střední uměleckoprůmyslovou školu sklářskou v Železném Brodě a v letech 1967-1973 studoval na Vysoké škole uměleckoprůmyslové v Praze v ateliéru prof. Stanislava Libenského. Je členem Umělecké besedy.[7] Jeho synové Lukáš a Ondřej jsou rovněž výtvarníci.[8] Většina objektů Pavla Trnky vychází z čistých geometrických tvarů. Optika skla přitom umožňuje přenášet barvu nebo světlo z jednoho prostoru do jiného jako při vytváření scény. Přiložením broušeného hranolu z optického skla k neonové trubici vytváří Trnka optickou iluzi přerušeného světelného paprsku a jeho přenosu do paralelních rovin.[22] Jeho dvoumetrový objekt z cyklu Spektrum (1982-1998), vytvořený z tvrzeného, broušeného a leštěného skla, zdobí arizonské Muzeum moderního umění.[2] Klasická jednoduchost a geometrická čistota broušených tvarů vytváří polaritu s množstvím asociovaných významů světelného kinetického děje.[21] V monumentálním objektu, který byl původně umístěn jako výzdoba stanice metra Národní, využil Trnka jako kinetický prvek proud vody, který prosvěcovala barevná světla řízená počítačovým programem. V objektu z cyklu Světlo, stín, čas jsou aktivními prvky zdroje barevného světla, hodiny s vteřinovou ručičkou a kyvadlo, ale hlavním kinetickým objektem se stává samotný divák, který vytváří mnohonásobné barevné stíny.[23] Během pobytu v Japonsku ovlivnily Pavla Trnku principy jógy i východní způsob života. V jeho obrazech se tato zkušenost projevila oproštěnou formou,[24] meditativním zklidněním a barevnými kombinacemi, které vnášejí harmonii a radost do všedního bytí. Skladba barev je založena na vyvážení nebo překrývání tónů a na diváka působí intenzitou a koncentrovaností výrazu. Barevné kombinace nejsou náhodné a podobně jako v hudbě z nich autor skládá nekonečné množství variací.[1] Pavel Trnka (* 2. září 1948, Poděbrady) je sklářský výtvarník, sochař, designér, malíř a pedagog. ']\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "combined_texts = defaultdict(str)\n",
    "\n",
    "for row in dataset['train']:\n",
    "    title = row['title']\n",
    "    text = row['text']\n",
    "    combined_texts[title] += text + \" \"\n",
    "\n",
    "combined_text_list = list(combined_texts.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "dddcf77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Information/Explanation',\n",
       " 'Information/Explanation',\n",
       " 'Information/Explanation',\n",
       " 'Information/Explanation',\n",
       " 'Information/Explanation']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, logit_output = model.predict(combined_text_list[:5])\n",
    "predictions\n",
    "\n",
    "\n",
    "[model.config.id2label[i] for i in predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a7d385",
   "metadata": {},
   "source": [
    "Output makes sense because these are articles from wikipedia."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
